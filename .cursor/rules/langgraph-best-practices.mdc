---
alwaysApply: true
description: LangGraph agent architecture and best practices for development
---

# LangGraph Best Practices

Follow these LangGraph best practices for building robust, maintainable agent systems.

## Agent Architecture Principles

### Single Responsibility Agents
- Each agent handles **ONE specific domain** (research, chat, coding, weather, etc.)
- Keep agent files under **500 lines** - split complex agents into focused modules
- Use [backend/services/langgraph_agents/base_agent.py](mdc:backend/services/langgraph_agents/base_agent.py) as the foundation
- Store agent logic in [backend/services/langgraph_agents/](mdc:backend/services/langgraph_agents/) directory

### Structured Agent Communication
- **ALWAYS use Pydantic models** from [backend/models/agent_response_models.py](mdc:backend/models/agent_response_models.py)
- **NEVER use string matching** for agent decisions (`if "permission" in response.lower()`)
- Include JSON schema in agent prompts for structured outputs
- Parse responses with Pydantic validation, handle `ValidationError` gracefully

Example structured response:
```python
from models.agent_response_models import ResearchTaskResult

try:
    structured_result = ResearchTaskResult.parse_raw(llm_response)
    state["agent_results"] = structured_result.dict()
except ValidationError as e:
    logger.error(f"Failed to parse structured response: {e}")
    # Graceful fallback logic
```

## LangGraph State Management

### Conversation State Pattern
- Use [backend/services/langgraph_enhanced_state.py](mdc:backend/services/langgraph_enhanced_state.py) for state definitions
- Store cross-agent data in `state["shared_memory"]` 
- Use `state["agent_results"]` for current agent outputs
- Maintain conversation history in `state["messages"]`

### Persistence with PostgreSQL
- Use [backend/services/langgraph_postgres_checkpointer.py](mdc:backend/services/langgraph_postgres_checkpointer.py)
- **NEVER implement custom memory stores** - use LangGraph's built-in checkpointing
- Enable state persistence for conversation continuity

## Human-in-the-Loop (HITL) Best Practices

### Interrupt-Before Pattern
```python
# In orchestrator workflow definition
workflow.add_conditional_edges(
    "research_agent",
    lambda state: "web_search_permission" if needs_permission(state) else "continue",
    {
        "web_search_permission": "web_search_permission",
        "continue": "format_response"
    }
)

# Add interrupt before permission nodes
workflow.add_node("web_search_permission", self._web_search_permission_node)
workflow = workflow.compile(
    checkpointer=checkpointer,
    interrupt_before=["web_search_permission"]
)
```

### Permission Request Pattern
- Create clear permission messages in agent responses
- Store permission requests in `state["agent_results"]["permission_request"]`
- Use explicit approval keywords: `["yes", "y", "ok", "proceed", "approved"]`
- Let LangGraph's `continue()` method handle flow resumption

### Resumption Logic
```python
# Simple resumption - let LangGraph handle the mechanics
if has_pending_permission and user_approves:
    input_data = {"messages": [HumanMessage(content=user_message)]}
    # LangGraph's interrupt_before + continue handles the rest
```

## Tool Registry and Access Control

### Tool Documentation
- **See [agent-tools-reference.mdc](mdc:.cursor/rules/agent-tools-reference.mdc) for complete tool catalog**
- All available tools are documented with usage examples
- Tools are categorized by type (gRPC-backed vs pure utilities)

### Tool Placement (Local vs Tools Service)
- **See [langgraph-tool-placement.mdc](mdc:.cursor/rules/langgraph-tool-placement.mdc) for the full rule**
- **Local to LLM Orchestrator**: Tools that only use in-memory data or strings (math, parsing, formatting, LLM-in-process) â€” no gRPC, no backend_tool_client
- **External**: Tools that touch database, vector store, web, email, files, or any service outside the orchestrator â€” implementation in Tools Service ([backend/services/grpc_tool_service.py](mdc:backend/services/grpc_tool_service.py)), orchestrator has thin gRPC wrappers only

### Centralized Tool Management
- Use [backend/services/langgraph_tools/centralized_tool_registry.py](mdc:backend/services/langgraph_tools/centralized_tool_registry.py)
- Group tools by domain in [backend/services/langgraph_tools/](mdc:backend/services/langgraph_tools/) directory
- Provide async wrapper functions for class methods to work with registry

Example tool wrapper:
```python
# In tool modules like web_content_tools.py
_web_content_instance = None

async def search_web(query: str, max_results: int = 5) -> str:
    global _web_content_instance
    if _web_content_instance is None:
        _web_content_instance = WebContentTools()
    return await _web_content_instance.search_web(query, max_results)
```

### Permission-Aware Tool Execution
- Check `shared_memory["web_search_permission"]` before web tools
- Implement graceful degradation for restricted tools
- Use local-first search strategy (try local knowledge before web)

## Orchestrator Best Practices

### Single Official Orchestrator
- Use [backend/services/langgraph_official_orchestrator.py](mdc:backend/services/langgraph_official_orchestrator.py) as the **ONLY** orchestrator
- **NEVER create multiple competing orchestrators**
- Route all requests through [backend/api/async_orchestrator_api.py](mdc:backend/api/async_orchestrator_api.py)

### Intent Classification and Routing
- Use structured intent analysis with `SmartQueryAnalyzer`
- Route based on intent types: `RESEARCH`, `CHAT`, `DIRECT`, `PERMISSION_GRANT`
- Store routing decisions in `state["intent_result"]["routing_recommendation"]`

### Conditional Edge Patterns
```python
def _route_from_intent(self, state: Dict[str, Any]) -> str:
    intent_result = state.get("intent_result", {})
    routing_recommendation = intent_result.get("routing_recommendation", "chat_agent")
    
    # Use agent recommendations, not hardcoded logic
    return routing_recommendation
```

## Agent Development Standards

### Editor Agent JSON Schema Pattern (CRITICAL)

**For ALL agents that return ManuscriptEdit structured output**, see [editor-agent-json-schema-pattern.mdc](mdc:.cursor/rules/editor-agent-json-schema-pattern.mdc) for detailed requirements.

### Editor Operations Reference (CRITICAL)

**For ALL agents that use editor operations** (fiction_editing_agent, outline_editing_agent, character_development_agent, rules_editing_agent, etc.), see [editor-operations-reference.mdc](mdc:.cursor/rules/editor-operations-reference.mdc) for complete documentation of available operations.

**Available Operations**:
- `insert_after_heading` - Add content below headers (SAFEST for structured documents)
- `insert_after` - Continue text mid-paragraph or mid-sentence
- `replace_range` - Replace existing content (granular edits)
- `delete_range` - Remove content

**Critical Rules**:
- âœ… NEVER include headers in `original_text` for `replace_range`/`delete_range`
- âœ… Use `insert_after_heading` when adding content below headers
- âœ… Provide EXACT, VERBATIM text from file (minimum 10-20 words)
- âœ… Always use centralized resolver: `resolve_editor_operation()` from `orchestrator.utils.editor_operation_resolver`
- âœ… Trust resolver to protect frontmatter automatically

**Key requirements:**
- âœ… Show VALID JSON example (not schema notation)
- âŒ NEVER include fake `"type": "ManuscriptEdit"` field
- âœ… Use field definitions AFTER the example
- âœ… Reinforce structure at both system prompt and generation time
- âœ… Include explicit warnings about common mistakes

**Applies to:** fiction_editing_agent, outline_editing_agent, rules_editing_agent, character_development_agent, podcast_script_agent (editing mode), substack_agent (editing mode)

### LLM Orchestrator Agent Requirements (CRITICAL)

**All agents in llm-orchestrator MUST follow these patterns:**

#### 1. BaseAgent Extension
- **ALWAYS extend** `orchestrator.agents.base_agent.BaseAgent`
- **NEVER create ChatOpenAI directly** - always use `self._get_llm(temperature=X, state=state)`
- **NEVER use chat_service.openai_client** - use centralized `_get_llm()` method
- **NEVER create custom LLM instances** in `__init__` - use `_get_llm()` for all LLM calls

#### 2. Workflow Structure (MANDATORY)
Every agent MUST implement:
```python
def _build_workflow(self, checkpointer) -> StateGraph:
    workflow = StateGraph(YourState)
    
    # Add nodes (minimum 2, typically 3-5)
    workflow.add_node("prepare_context", self._prepare_context_node)
    workflow.add_node("process_request", self._process_request_node)
    workflow.add_node("format_response", self._format_response_node)
    
    # Set entry point
    workflow.set_entry_point("prepare_context")
    
    # Define edges
    workflow.add_edge("prepare_context", "process_request")
    workflow.add_edge("process_request", "format_response")
    workflow.add_edge("format_response", END)
    
    # ALWAYS compile with checkpointer
    return workflow.compile(checkpointer=checkpointer)
```

#### 3. State TypedDict (REQUIRED)
```python
class YourAgentState(TypedDict):
    """State for your agent LangGraph workflow"""
    query: str
    user_id: str
    metadata: Dict[str, Any]
    messages: List[Any]
    shared_memory: Dict[str, Any]  # For cross-agent context
    # ... agent-specific fields ...
    response: Dict[str, Any]
    task_status: str
    error: str
```

#### 4. Node Naming Convention
- **ALL nodes MUST follow pattern**: `async def _*_node(self, state: YourState) -> Dict[str, Any]:`
- Node names should be descriptive: `_prepare_context_node`, `_analyze_content_node`, `_format_response_node`
- Nodes return state updates as dictionary

#### 5. Node Error Handling Pattern
```python
async def _your_node(self, state: YourState) -> Dict[str, Any]:
    try:
        # Node logic here
        return {
            "field_name": result,
            "task_status": "complete"
        }
    except Exception as e:
        logger.error(f"Error in _your_node: {e}")
        return {
            "error": str(e),
            "task_status": "error"
        }
```

#### 6. Process Method Pattern (STANDARD)
All agents MUST implement `process()` following this pattern:
```python
async def process(self, query: str, metadata: Dict[str, Any] = None, messages: List[Any] = None) -> Dict[str, Any]:
    try:
        # Get workflow (lazy initialization with checkpointer)
        workflow = await self._get_workflow()
        
        # Extract user_id from metadata
        metadata = metadata or {}
        user_id = metadata.get("user_id", "system")
        
        # Get checkpoint config (handles thread_id from conversation_id/user_id)
        config = self._get_checkpoint_config(metadata)
        
        # Prepare new messages (current query)
        new_messages = self._prepare_messages_with_query(messages, query)
        
        # Load and merge checkpointed messages to preserve conversation history
        conversation_messages = await self._load_and_merge_checkpoint_messages(
            workflow, config, new_messages
        )
        
        # Load shared_memory from checkpoint if available
        checkpoint_state = await workflow.aget_state(config)
        existing_shared_memory = {}
        if checkpoint_state and checkpoint_state.values:
            existing_shared_memory = checkpoint_state.values.get("shared_memory", {})
        
        # Merge with any shared_memory from metadata
        shared_memory = metadata.get("shared_memory", {}) or {}
        shared_memory.update(existing_shared_memory)
        
        # Initialize state for LangGraph workflow
        initial_state: YourAgentState = {
            "query": query,
            "user_id": user_id,
            "metadata": metadata,
            "messages": conversation_messages,
            "shared_memory": shared_memory,
            # ... other required fields ...
            "response": {},
            "task_status": "",
            "error": ""
        }
        
        # Run LangGraph workflow with checkpointing
        result_state = await workflow.ainvoke(initial_state, config=config)
        
        # Extract final response
        response = result_state.get("response", {})
        task_status = result_state.get("task_status", "complete")
        
        if task_status == "error":
            error_msg = result_state.get("error", "Unknown error")
            logger.error(f"âŒ {self.agent_type} failed: {error_msg}")
            return self._create_error_response(error_msg)
        
        return response
        
    except Exception as e:
        logger.error(f"âŒ {self.agent_type} failed: {e}")
        return self._create_error_response(str(e))
```

#### 7. LLM Call Pattern (MANDATORY)
```python
# âœ… CORRECT: Use centralized _get_llm()
llm = self._get_llm(temperature=0.7, state=state)
response = await llm.ainvoke(messages)

# âŒ WRONG: Direct ChatOpenAI creation
self.llm = ChatOpenAI(...)  # NEVER DO THIS

# âŒ WRONG: Using chat_service
response = await chat_service.openai_client.chat.completions.create(...)  # NEVER DO THIS
```

#### 8. Checkpoint and State Management
- **ALWAYS use** `_get_checkpoint_config(metadata)` for thread_id management
- **ALWAYS use** `_load_and_merge_checkpoint_messages()` for conversation history
- **ALWAYS merge** shared_memory from checkpoints with metadata **IN CORRECT ORDER** (see below)
- **ALWAYS pass** `config=config` to `workflow.ainvoke()`

#### 8a. âš ï¸ CRITICAL: Checkpoint Merge Order Pattern (MANDATORY)

**THE BUG:** Merging `shared_memory` in wrong order causes stale checkpoint data to overwrite fresh request data!

**The Problem:**
```python
# âŒ WRONG ORDER - CAUSES STALE CONTENT BUG!
checkpoint_state = await workflow.aget_state(config)
existing_shared_memory = checkpoint_state.values.get("shared_memory", {}) if checkpoint_state and checkpoint_state.values else {}

# Start with fresh data
shared_memory = metadata.get("shared_memory", {}) or {}
# Then let stale checkpoint overwrite it
shared_memory.update(existing_shared_memory)  # âŒ OLD OVERWRITES NEW!

# Result: active_editor has STALE content from previous turn!
```

**Real-World Bug Example:**
1. **Request 1**: File has 140 chars (frontmatter only) â†’ saved to checkpoint
2. **Request 2**: File now has 2163 chars â†’ sent in fresh `active_editor`
3. **Wrong merge**: Stale 140-char checkpoint overwrites fresh 2163-char data
4. **Agent behavior**: Only sees 140 chars â†’ treats file as empty â†’ recreates content instead of editing!

**The Fix (MANDATORY):**
```python
# âœ… CORRECT ORDER - FRESH DATA OVERWRITES STALE!
checkpoint_state = await workflow.aget_state(config)
existing_shared_memory = {}
if checkpoint_state and checkpoint_state.values:
    existing_shared_memory = checkpoint_state.values.get("shared_memory", {})

# Start with checkpoint (old/stale)
shared_memory_merged = existing_shared_memory.copy()
# Then let fresh metadata overwrite stale checkpoint
shared_memory_merged.update(metadata.get("shared_memory", {}) or {})  # âœ… NEW OVERWRITES OLD!

# Use shared_memory_merged in initial_state
initial_state = {
    "shared_memory": shared_memory_merged,  # âœ… Fresh active_editor content!
    # ... other fields
}
```

**Why This Matters:**
- `active_editor` contains **current file content** from request
- Checkpoint may have **stale `active_editor`** from previous conversation turn
- Wrong order = agent sees old file state, creates duplicates, loses user edits
- **This is not a theoretical edge case** - this bug was discovered in production!

**Rule of Thumb:**
- **Checkpoint data = STALE** (from previous turn)
- **Metadata data = FRESH** (from current request)
- **ALWAYS let fresh overwrite stale**

**Applies to ALL agents using checkpointing**, especially:
- Writing Assistant
- Character Development Agent
- Fiction Editing Agent
- Any agent accessing `active_editor`

**See:** `dev-notes/AGENT_INTEGRATION_GUIDE.md` for detailed bug investigation and examples.

#### 9. Workflow Requirements Checklist
- [ ] Inherits from `BaseAgent`
- [ ] Defines `TypedDict` state class
- [ ] Implements `_build_workflow(checkpointer)` method
- [ ] Creates `StateGraph(YourState)`
- [ ] Adds minimum 2 nodes (typically 3-5)
- [ ] Sets entry point with `workflow.set_entry_point()`
- [ ] Defines edges with `workflow.add_edge()` or conditional edges
- [ ] Compiles with `workflow.compile(checkpointer=checkpointer)`
- [ ] All nodes follow `_*_node` naming pattern
- [ ] All nodes return `Dict[str, Any]`
- [ ] **Each node preserves critical state** (see State Preservation below)
- [ ] Implements `process()` method with standard pattern
- [ ] Uses `_get_llm()` for all LLM calls
- [ ] Handles errors gracefully in nodes

#### 10. State Preservation Across Nodes (CRITICAL)

**LangGraph State Behavior**: When a node returns a dict, it **UPDATES** the state. **Keys not in the return dict are DROPPED!**

**Problem Pattern:**
```python
# âŒ BAD: Node B drops active_editor
async def _node_a(self, state: YourState) -> Dict[str, Any]:
    active_editor = state.get("active_editor", {})
    # Process...
    return {
        "active_editor": active_editor,  # Set in state
        "result_a": "data"
    }

async def _node_b(self, state: YourState) -> Dict[str, Any]:
    # Can access active_editor here
    active_editor = state.get("active_editor", {})
    # Process...
    return {
        "result_b": "data"
        # âŒ active_editor NOT returned â†’ DROPPED FROM STATE!
    }

async def _node_c(self, state: YourState) -> Dict[str, Any]:
    active_editor = state.get("active_editor", {})
    # âŒ active_editor is {} or missing - was dropped by node_b!
```

**Solution Pattern:**
```python
# âœ… GOOD: Each node preserves critical state
async def _node_b(self, state: YourState) -> Dict[str, Any]:
    active_editor = state.get("active_editor", {})
    # Process...
    return {
        "result_b": "data",
        "active_editor": state.get("active_editor", {}),  # âœ… Preserve for next node
        "shared_memory": state.get("shared_memory", {}),  # âœ… Preserve
        "messages": state.get("messages", []),  # âœ… Preserve
        "user_id": state.get("user_id", "system"),  # âœ… Preserve
        "metadata": state.get("metadata", {}),  # âœ… Preserve
        "query": state.get("query", "")  # âœ… Preserve
    }
```

**Critical State Keys to Preserve:**
- `metadata` - **MUST preserve for user model preference!** Contains `user_chat_model` used by `_get_llm()`
- `user_id` - **MUST preserve for database queries** (folder lookups, document access)
- `active_editor` - Editor context with content, frontmatter, cursor
- `shared_memory` - Cross-agent and cross-turn context
- `messages` - Conversation history
- `query` - Original user query
- `manuscript` / `filename` / `frontmatter` - Document context (fiction agents)

**CRITICAL FAILURES:**
- Missing `metadata` â†’ LLM falls back to DEFAULT_MODEL instead of user's chosen model!
- Missing `user_id` â†’ Database queries use "system" as fallback and fail to find user-owned documents/folders!

**Best Practice:**
1. **Identify critical state** - What does this workflow need across all nodes?
2. **Every node returns critical state** - Even if unchanged
3. **Error paths too** - Preserve state even in exception handlers
4. **Subgraphs especially** - State flows through multiple internal nodes

**Testing State Preservation:**
- Add debug logging: `logger.info(f"Node X state keys: {list(state.keys())}")`
- Check each node's return dict includes critical keys
- Verify downstream nodes receive expected state
- **Test with user model preference** - Ensure subgraphs use correct model, not default!

### Subgraph State Preservation (CRITICAL FOR ALL SUBGRAPHS)

**Problem:** Subgraphs that don't preserve critical state cause cascading failures in parent workflows.

**Real-World Example - Model Fallback Bug:**
```
Parent Agent: uses anthropic/claude-sonnet-4.5 (user's choice) âœ…
  â†“
Generation Subgraph Node 1: returns {output: "data"} âŒ Drops metadata!
  â†“
Generation Subgraph Node 2: state.get("metadata") â†’ None
  â†“
_get_llm(state=state): No metadata â†’ Falls back to DEFAULT_MODEL âŒ
  â†“
Result: Wrong model used! (google/gemini-2.5-pro instead of user's Claude choice)
```

**MANDATORY Pattern for ALL Subgraph Nodes:**

Every node in a subgraph must return these 5 critical keys:

```python
async def any_subgraph_node(state: Dict[str, Any]) -> Dict[str, Any]:
    try:
        # ... node logic ...
        
        return {
            # Node-specific outputs
            "node_output": result,
            "is_complete": True,
            
            # âœ… CRITICAL: ALWAYS preserve these 5 keys
            "metadata": state.get("metadata", {}),      # User model preference!
            "user_id": state.get("user_id", "system"),  # Database access!
            "shared_memory": state.get("shared_memory", {}),
            "messages": state.get("messages", []),
            "query": state.get("query", ""),
        }
        
    except Exception as e:
        logger.error(f"Node failed: {e}")
        return {
            "error": str(e),
            "task_status": "error",
            
            # âœ… CRITICAL: Preserve even on error!
            "metadata": state.get("metadata", {}),
            "user_id": state.get("user_id", "system"),
            "shared_memory": state.get("shared_memory", {}),
            "messages": state.get("messages", []),
            "query": state.get("query", ""),
        }
```

**Why Each Key Matters:**

| Key | Why Critical | What Breaks If Missing |
|-----|--------------|----------------------|
| `metadata` | Contains `user_chat_model` | LLM calls fall back to DEFAULT_MODEL |
| `user_id` | Database query context | Queries fail with "system" user (no access to user's documents) |
| `shared_memory` | Cross-agent context | Agent continuity broken, loses conversation state |
| `messages` | Conversation history | LLM loses context, can't reference previous turns |
| `query` | Original user request | Downstream nodes can't access user's intent |

**Subgraph Construction Checklist:**

When building a subgraph:
- [ ] Identify ALL nodes in the subgraph
- [ ] For EACH node (including error handlers):
  - [ ] Add the 5 critical keys to return dict (success path)
  - [ ] Add the 5 critical keys to return dict (error path)
  - [ ] Use `state.get(key, default)` pattern for safety
- [ ] Test the subgraph with user model preference set
- [ ] Verify model doesn't fall back to default
- [ ] Verify database queries succeed

**Common Subgraph Anti-Patterns:**

```python
# âŒ BAD: Only returns node-specific output
return {"context_parts": parts}

# âŒ BAD: Only preserves some critical keys
return {
    "context_parts": parts,
    "user_id": state.get("user_id"),  # Missing metadata!
}

# âŒ BAD: Success path preserves, error path doesn't
try:
    return {"result": data, "metadata": state.get("metadata", {})}
except:
    return {"error": str(e)}  # Missing metadata!

# âœ… GOOD: All critical keys in success AND error paths
try:
    return {
        "result": data,
        "metadata": state.get("metadata", {}),
        "user_id": state.get("user_id", "system"),
        "shared_memory": state.get("shared_memory", {}),
        "messages": state.get("messages", []),
        "query": state.get("query", ""),
    }
except Exception as e:
    return {
        "error": str(e),
        "task_status": "error",
        "metadata": state.get("metadata", {}),
        "user_id": state.get("user_id", "system"),
        "shared_memory": state.get("shared_memory", {}),
        "messages": state.get("messages", []),
        "query": state.get("query", ""),
    }
```

**Debugging State Loss in Subgraphs:**

If you see unexpected behavior (wrong model, database failures, etc.):

1. **Check for state dropping**: Add logging at each node entry/exit
2. **Verify critical keys**: Log `list(state.keys())` at each node
3. **Test model selection**: Add logging in `_get_llm()` to show which model is selected
4. **Trace the path**: Follow state through each node in the subgraph

**Real-World Fix Example (fiction_generation_subgraph):**

Before (wrong model used):
```python
# Node returned only its output
return {"generation_context_parts": parts}
# Result: metadata dropped, LLM falls back to default model
```

After (correct model used):
```python
return {
    "generation_context_parts": parts,
    "metadata": state.get("metadata", {}),  # âœ… User model preserved!
    "user_id": state.get("user_id", "system"),
    "shared_memory": state.get("shared_memory", {}),
    "messages": state.get("messages", []),
    "query": state.get("query", ""),
}
# Result: User's chosen model (Claude) used correctly!
```

**Validation Subgraph Example (fiction_validation_subgraph):**

Validation subgraphs must preserve BOTH critical 5 keys AND domain-specific context:

```python
async def validate_consistency_node(state: Dict[str, Any]) -> Dict[str, Any]:
    try:
        # ... validation logic ...
        return {
            "consistency_warnings": warnings,
            # âœ… CRITICAL 5
            "metadata": state.get("metadata", {}),
            "user_id": state.get("user_id", "system"),
            "shared_memory": state.get("shared_memory", {}),
            "messages": state.get("messages", []),
            "query": state.get("query", ""),
            # âœ… Domain-specific context (fiction agents)
            "manuscript": state.get("manuscript", ""),
            "filename": state.get("filename", ""),
            "current_chapter_text": state.get("current_chapter_text", ""),
            "current_chapter_number": state.get("current_chapter_number"),
            "chapter_ranges": state.get("chapter_ranges", []),
            "current_request": state.get("current_request", ""),
            "active_editor": state.get("active_editor", {}),
            "frontmatter": state.get("frontmatter", {}),
            "structured_edit": state.get("structured_edit"),
        }
    except Exception as e:
        return {
            "consistency_warnings": [],
            # âœ… Preserve ALL keys even on error!
            "metadata": state.get("metadata", {}),
            "user_id": state.get("user_id", "system"),
            "shared_memory": state.get("shared_memory", {}),
            "messages": state.get("messages", []),
            "query": state.get("query", ""),
            "manuscript": state.get("manuscript", ""),
            "filename": state.get("filename", ""),
            "current_chapter_text": state.get("current_chapter_text", ""),
            "current_chapter_number": state.get("current_chapter_number"),
            "chapter_ranges": state.get("chapter_ranges", []),
            "current_request": state.get("current_request", ""),
            "active_editor": state.get("active_editor", {}),
            "frontmatter": state.get("frontmatter", {}),
            "structured_edit": state.get("structured_edit"),
        }
```

**Resolution Subgraph Example (fiction_resolution_subgraph):**

Resolution subgraphs use prefixed state keys internally but must preserve BOTH original AND prefixed keys:

```python
async def resolve_individual_operations_node(state: Dict[str, Any]) -> Dict[str, Any]:
    try:
        # Uses prefixed keys internally
        manuscript = state.get("resolution_manuscript", "")
        operations = state.get("resolution_operations", [])
        # ... resolution logic ...
        
        return {
            "resolved_operations": editor_operations,
            "resolution_complete": False,
            # âœ… CRITICAL 5
            "metadata": state.get("metadata", {}),
            "user_id": state.get("user_id", "system"),
            "shared_memory": state.get("shared_memory", {}),
            "messages": state.get("messages", []),
            "query": state.get("query", ""),
            # âœ… Domain-specific context
            "manuscript": state.get("manuscript", ""),
            "filename": state.get("filename", ""),
            "current_chapter_text": state.get("current_chapter_text", ""),
            "current_chapter_number": state.get("current_chapter_number"),
            "chapter_ranges": state.get("chapter_ranges", []),
            "current_request": state.get("current_request", ""),
            "active_editor": state.get("active_editor", {}),
            "frontmatter": state.get("frontmatter", {}),
            "structured_edit": state.get("structured_edit"),
            # âœ… Prefixed keys (for next nodes in subgraph)
            "resolution_manuscript": state.get("resolution_manuscript", ""),
            "resolution_structured_edit": state.get("resolution_structured_edit"),
            "resolution_operations": state.get("resolution_operations", []),
        }
    except Exception as e:
        return {
            "resolved_operations": [],
            "error": str(e),
            "task_status": "error",
            "resolution_complete": True,
            # âœ… Preserve ALL keys even on error!
            "metadata": state.get("metadata", {}),
            "user_id": state.get("user_id", "system"),
            "shared_memory": state.get("shared_memory", {}),
            "messages": state.get("messages", []),
            "query": state.get("query", ""),
            "manuscript": state.get("manuscript", ""),
            "filename": state.get("filename", ""),
            "current_chapter_text": state.get("current_chapter_text", ""),
            "current_chapter_number": state.get("current_chapter_number"),
            "chapter_ranges": state.get("chapter_ranges", []),
            "current_request": state.get("current_request", ""),
            "active_editor": state.get("active_editor", {}),
            "frontmatter": state.get("frontmatter", {}),
            "structured_edit": state.get("structured_edit"),
            "resolution_manuscript": state.get("resolution_manuscript", ""),
            "resolution_structured_edit": state.get("resolution_structured_edit"),
            "resolution_operations": state.get("resolution_operations", []),
        }
```

**Key Takeaway:** Subgraphs that operate on domain-specific data (like fiction manuscripts) must preserve BOTH the critical 5 keys AND all domain context keys. Missing domain context causes downstream nodes to lose critical information (e.g., `chapter_ranges` being empty makes LLM think file is empty).

### Standardized Image Handling in Subgraphs (CRITICAL)

**ALL subgraphs that handle images MUST use the standardized image emission pattern.**

**Required Pattern for Image-Emitting Subgraphs:**

1. **Image Search Tools Return Structured Data:**
   ```python
   # Image search tools return:
   {
       "images_markdown": "![]({data_uri})\n![]({data_uri})...",  # For display
       "metadata": [{title, date, series, author, ...}],         # For LLM context
       "images": [{url, alt_text, type, metadata}]                # For AgentResponse
   }
   ```

2. **Subgraphs Extract and Pass Through Both Formats:**
   ```python
   # In intelligent_document_retrieval_subgraph or image_search_subgraph
   image_search_result = await search_images_intelligently(...)
   
   # Extract both formats
   images_markdown = image_search_result.get("images_markdown", "")
   structured_images = image_search_result.get("images", [])
   
   return {
       "image_search_results": images_markdown,  # Markdown for backward compatibility
       "structured_images": structured_images,   # Structured data for AgentResponse
       # ... preserve critical state keys ...
   }
   ```

3. **Research Workflow Subgraphs Collect from Parallel Searches:**
   ```python
   # Collect structured images from all parallel search results
   all_structured_images = []
   all_image_results = []
   
   for result in parallel_search_results:
       if result.get("structured_images"):
           all_structured_images.extend(result.get("structured_images"))
       if result.get("image_search_results"):
           all_image_results.append(result.get("image_search_results"))
   
   return {
       "round1_results": {
           "image_search_results": "\n\n".join(all_image_results),  # Combined markdown
           "structured_images": all_structured_images                 # Combined structured data
       },
       # ... preserve critical state keys ...
   }
   ```

4. **Agents Include Both in Final AgentResponse:**
   ```python
   # In agent synthesis/format nodes
   structured_images = round1_results.get("structured_images")
   image_search_results = round1_results.get("image_search_results")
   
   # Append markdown to response text
   final_response = f"{text_response}"
   if image_search_results:
       final_response = f"{final_response}\n\n{image_search_results}"
   
   # Include in AgentResponse
   standard_response = AgentResponse(
       response=final_response,  # Text + markdown images
       task_status="complete",
       agent_type="your_agent",
       timestamp=datetime.now().isoformat(),
       images=structured_images if structured_images else None  # Structured data
   )
   ```

**Why This Pattern?**
- **Markdown in response**: Frontend extracts images from markdown for immediate chat display
- **Structured `images` field**: Provides full metadata, enables image galleries, tooltips, programmatic access
- **Backward compatibility**: Existing frontend code continues to work
- **Future enhancements**: Structured data enables rich image features (galleries, metadata tooltips, etc.)

**State Preservation for Images:**
- **ALWAYS preserve** `structured_images` through ALL subgraph nodes (even if not used)
- **ALWAYS preserve** `image_search_results` (markdown) through ALL subgraph nodes
- **ALWAYS include both** in final AgentResponse (markdown in `response` text + structured in `images` field)

**See [agent-response-contract.mdc](mdc:.cursor/rules/agent-response-contract.mdc) for complete image emission documentation.**

## LangGraph State Flow Principles (CRITICAL - HARD-WON LESSONS)

### Principle 1: Returns Don't Exit Graphs! (CRITICAL!)

**In LangGraph, returning from a node does NOT exit the graph!** The return dict updates state, and execution continues to the next node defined by edges.

**Problem Pattern:**
```python
# âŒ WRONG ASSUMPTION: This does NOT exit the graph!
async def _vector_search_node(state):
    # Do image search
    image_results = await search_images(...)
    
    if image_results:
        return {
            "formatted_context": "Found 3 comics...",  # Set valuable state
            "image_search_results": image_results
        }
        # âŒ Graph CONTINUES to next node, which might overwrite this!
    
    # Regular document search...
    return {"formatted_context": "...", "retrieved_documents": [...]}

# Later nodes in the graph:
async def _format_context_node(state):
    if not state.get("retrieved_documents"):
        # âŒ This OVERWRITES the formatted_context from image search!
        return {"formatted_context": ""}  # Destroys valuable data!
```

**Correct Solutions:**

**Option A: Use Conditional Routing**
```python
# âœ… CORRECT: Route to END when done
workflow.add_conditional_edges(
    "vector_search",
    lambda state: "END" if state.get("image_search_results") else "format_context",
    {
        "END": END,
        "format_context": "format_context"
    }
)
```

**Option B: Preserve Existing State**
```python
# âœ… CORRECT: Don't blindly overwrite
async def _format_context_node(state):
    if not state.get("retrieved_documents"):
        # Preserve existing formatted_context if it exists
        existing_context = state.get("formatted_context", "")
        return {"formatted_context": existing_context}  # Keep it!
```

**Rule**: Plan your graph flow carefully. If a node might set final state, either route to END immediately or ensure all downstream nodes preserve that state.

### Principle 2: Never Blindly Overwrite State Fields

Before setting a state field to a default/empty value, check if it already contains valuable data:

```python
# âŒ WRONG: Destroys existing state
async def _some_node(state):
    if not my_condition:
        return {
            "formatted_context": "",  # Overwrites any existing value!
            "result": None
        }

# âœ… CORRECT: Preserve existing values
async def _some_node(state):
    if not my_condition:
        return {
            "formatted_context": state.get("formatted_context", ""),  # Keep existing!
            "result": state.get("result")  # Preserve
        }
```

**Rule**: Only overwrite a state field when you have NEW data for it. Otherwise, preserve the existing value with `state.get("field", default)`.

### Principle 3: Identify ALL Workflow State Keys

Beyond the critical 5 keys (metadata, user_id, shared_memory, messages, query), identify EVERY state key your workflow might use:

**Workflow State Audit Checklist:**
1. **List all nodes in your workflow/subgraph**
2. **For each node, identify what state keys it sets**
3. **Create master list of ALL possible state keys**
4. **Add ALL keys to EVERY node's return dict**

**Example - Chat Agent Workflow:**
```python
# Workflow: prepare_context â†’ check_local_data â†’ detect_calculations â†’ generate_response

# State keys used across workflow:
# - Critical 5: metadata, user_id, shared_memory, messages, query
# - Workflow-specific: local_data_results, calculation_result, needs_calculations, 
#   persona, system_prompt, llm_messages, extracted_images

# EVERY node must preserve ALL of these!
async def _any_node_in_chat_workflow(state: ChatState) -> Dict[str, Any]:
    # Node logic...
    
    return {
        # Node output
        "my_field": result,
        # âœ… CRITICAL 5
        "metadata": state.get("metadata", {}),
        "user_id": state.get("user_id", "system"),
        "shared_memory": state.get("shared_memory", {}),
        "messages": state.get("messages", []),
        "query": state.get("query", ""),
        # âœ… Chat workflow specific
        "local_data_results": state.get("local_data_results"),
        "calculation_result": state.get("calculation_result"),
        "needs_calculations": state.get("needs_calculations"),
        "persona": state.get("persona"),
        "system_prompt": state.get("system_prompt"),
        "llm_messages": state.get("llm_messages"),
        "extracted_images": state.get("extracted_images", [])
    }
```

### Principle 4: Graph Flow Continues Through ALL Nodes

When a subgraph has flow: `node_1 â†’ node_2 â†’ node_3 â†’ END`:
- If node_1 sets `state["special_data"]`
- Nodes 2 AND 3 MUST preserve it (even if they don't use it!)
- Otherwise it's DROPPED by the time you reach END

```python
# Subgraph with 3 nodes:
async def _node_1(state): 
    return {"formatted_context": "Important!", "result_1": "data"}

async def _node_2(state):
    return {"result_2": "more data"}  # âŒ Drops formatted_context!

async def _node_3(state):
    # formatted_context is now missing!
    return {"final": "result"}

# âœ… CORRECT:
async def _node_2(state):
    return {
        "result_2": "more data",
        "formatted_context": state.get("formatted_context", "")  # Preserve!
    }
```

### Principle 5: Debug State Loss with Logging

When state mysteriously disappears:

```python
# Add these logs to EVERY node during debugging:
async def _suspicious_node(state: YourState) -> Dict[str, Any]:
    logger.info(f"ðŸ“Š {node_name} ENTRY - state keys: {list(state.keys())}")
    
    # Node logic...
    return_dict = {
        "my_field": result,
        # ... state preservation ...
    }
    
    logger.info(f"ðŸ“¤ {node_name} EXIT - returning keys: {list(return_dict.keys())}")
    return return_dict
```

**Debug Process:**
1. Log state keys at ENTRY to each node
2. Log return dict keys at EXIT from each node
3. Compare: missing keys in EXIT = culprit found!
4. Check logs for where state keys suddenly disappear

### Real-World Example: Image Search Metadata Loss

**Problem:** Image search found comics with metadata, but LLM said "no comics found"

**Root Cause Discovery:**
```
âœ… Image search node: sets formatted_context = "Found 3 comics..."
   (Graph continues...)
âœ… Document retrieval node: preserves formatted_context
   (Graph continues...)
âŒ Format context node: sees no retrieved_documents, returns formatted_context = ""
   (OVERWRITES the image metadata!)
âœ… Final result: formatted_context is empty
âŒ Chat agent: sees empty formatted_context, LLM gets no metadata!
```

**Fix Applied:**
```python
async def _format_context_node(state):
    if not retrieved_documents:
        # âœ… FIX: Preserve existing formatted_context
        existing = state.get("formatted_context", "")
        return {"formatted_context": existing}  # Don't overwrite!
```

**Time to Find:** 3+ hours of debugging  
**Time to Fix:** 1 line of code  
**Lesson:** Always preserve existing state unless you have new data!

### Principle 6: Test State Preservation End-to-End

**Testing Checklist:**
- [ ] Add state key logging to all nodes
- [ ] Run workflow with complex state (multiple keys set)
- [ ] Verify state keys at END match what was set at beginning
- [ ] Check that user model preference (metadata) survives all nodes
- [ ] Verify database queries work (user_id preserved)
- [ ] Confirm formatted results reach the LLM

**Quick Test Pattern:**
```python
# Set distinctive state at start
initial_state = {
    "query": "test",
    "metadata": {"user_chat_model": "test-model"},
    "formatted_context": "TEST_VALUE_SHOULD_SURVIVE"
}

result = await workflow.ainvoke(initial_state)

# Verify it survived
assert result.get("formatted_context") == "TEST_VALUE_SHOULD_SURVIVE", \
    "State was dropped during graph execution!"
```

### Summary: State Preservation Commandments

1. **Returns don't exit** - Use conditional routing to skip nodes
2. **Never blindly overwrite** - Check existing values before defaulting
3. **Preserve ALL workflow keys** - Not just the critical 5
4. **Every node preserves everything** - Even fields it doesn't use
5. **Debug with logging** - Track state keys through the graph
6. **Test end-to-end** - Verify state survives the entire flow

**These principles prevent hours of debugging "disappearing data" bugs!**

### Base Agent Extension (Backend Agents)
- Extend [backend/services/langgraph_agents/base_agent.py](mdc:backend/services/langgraph_agents/base_agent.py)
- Override `_process_request()` method for agent-specific logic
- Use `self.tool_registry` for tool access
- Implement proper error handling and logging

### Frontmatter Reference Management (CRITICAL)

**When agents need to edit reference documents, they MUST have frontmatter knowledge loaded first.**

**Required Pattern:**
1. **ALWAYS load referenced files before file editing operations**
   - Use `load_referenced_files()` from `orchestrator.tools.reference_file_loader`
   - Store results in state as `referenced_context: Dict[str, Any]`
   - This ensures agents know what files exist in the project

2. **Workflow Order (MANDATORY):**
   ```python
   # âœ… CORRECT: Load context BEFORE file editing
   workflow.add_node("load_referenced_context", self._load_referenced_context_node)
   workflow.add_node("route_and_save_content", self._route_and_save_content_node)
   
   # Load context ALWAYS happens before file editing
   workflow.add_edge("load_referenced_context", "...")  # Goes through other nodes
   workflow.add_edge("...", "route_and_save_content")  # Eventually reaches file editing
   ```

3. **State Access Pattern:**
   ```python
   # In file editing nodes, ALWAYS access both:
   referenced_context = state.get("referenced_context", {})  # Loaded files with document_ids
   active_editor = metadata.get("shared_memory", {}).get("active_editor", {})
   frontmatter = active_editor.get("frontmatter", {})  # File references from frontmatter
   
   # Use frontmatter to know what files exist
   # Use referenced_context to resolve relative paths to document_ids
   ```

4. **Why This Matters:**
   - **Path Resolution**: Relative paths like `./component_list.md` must be resolved to document_ids
   - **File Discovery**: Agents need to know what files are part of the project
   - **Content Routing**: LLM routing decisions need available file list
   - **Frontmatter Updates**: When creating new files, must update project plan frontmatter

5. **Safeguard Pattern:**
   ```python
   # Always verify frontmatter is available before file operations
   if not frontmatter and not project_plan_document_id:
       logger.warning("âš ï¸ No frontmatter or project plan available - cannot resolve file references")
       return {}  # Skip file operations
   ```

**Example Implementation (from electronics_agent):**
```python
async def _load_referenced_context_node(self, state: ElectronicsState) -> Dict[str, Any]:
    """Load referenced files from active editor frontmatter"""
    active_editor = metadata.get("shared_memory", {}).get("active_editor", {})
    
    # Load files referenced in frontmatter
    result = await load_referenced_files(
        active_editor=active_editor,
        user_id=user_id,
        reference_config=reference_config
    )
    
    return {"referenced_context": result.get("loaded_files", {})}

async def _route_and_save_content_node(self, state: ElectronicsState) -> Dict[str, Any]:
    """Save content to files - REQUIRES referenced_context to be loaded first"""
    # Get frontmatter references (from active_editor)
    frontmatter = active_editor.get("frontmatter", {})
    
    # Get loaded files with document_ids (from referenced_context)
    referenced_context = state.get("referenced_context", {})
    
    # Use both to resolve paths and route content
    # ...
```

### Prompt Engineering
- Include system context and role definition
- Specify structured output requirements with JSON schema
- Provide clear examples of expected responses
- Use Roosevelt's speaking style for consistency: "BULLY!", "By George!"

### Testing and Validation
- Test agent interactions through orchestrator API
- Validate structured outputs with Pydantic
- Test HITL flows with interrupt/resume cycles
- Ensure state persistence across conversation turns

## Dynamic Subgraphs - Advanced Modularity Patterns

### When to Use Dynamic Subgraphs

**âœ… USE SUBGRAPHS FOR:**
- **Complex multi-step workflows** with 4+ related nodes
- **Reusable components** that multiple agents need
- **Parallel processing** of independent workflow segments
- **Domain isolation** where state schemas differ significantly
- **Permission workflows** that need consistent HITL patterns

**âŒ DON'T USE SUBGRAPHS FOR:**
- **Simple single-step operations** (over-engineering)
- **Tightly coupled linear flows** (unnecessary complexity)
- **Performance-critical paths** (avoid subgraph overhead)
- **One-off workflows** with no reuse potential

### Subgraph Implementation Patterns

#### Research Workflow Subgraph
```python
# Modular research pipeline
research_subgraph = StateGraph(ResearchState)
research_subgraph.add_node("local_search", local_search_node)
research_subgraph.add_node("permission_check", permission_check_node)
research_subgraph.add_node("web_search", web_search_node)
research_subgraph.add_node("synthesize_results", synthesis_node)

# Add to main graph as single node
main_graph.add_node("research_workflow", research_subgraph.compile())
```

#### Permission Management Subgraph
```python
# Reusable HITL permission workflow
permission_subgraph = StateGraph(PermissionState)
permission_subgraph.add_node("analyze_request", analyze_request_node)
permission_subgraph.add_node("request_permission", request_permission_node)
permission_subgraph.add_node("process_response", process_response_node)

# Configure interrupt_before for HITL
permission_graph = permission_subgraph.compile(
    interrupt_before=["request_permission"]
)
```

### State Management Between Graphs

#### Shared State Keys
```python
# When parent and subgraph share state schema
class SharedState(TypedDict):
    messages: List[BaseMessage]
    shared_memory: Dict[str, Any]
    agent_results: Dict[str, Any]

# Direct integration - no transformation needed
```

#### State Transformation
```python
# When schemas differ, implement transformation functions
def transform_to_subgraph_state(parent_state: ParentState) -> SubgraphState:
    return SubgraphState(
        query=parent_state["messages"][-1].content,
        context=parent_state["shared_memory"],
        permissions=parent_state.get("permissions", {})
    )

def merge_subgraph_results(parent_state: ParentState, subgraph_result: SubgraphState) -> ParentState:
    parent_state["agent_results"].update(subgraph_result.get("results", {}))
    parent_state["shared_memory"].update(subgraph_result.get("context", {}))
    return parent_state
```

### HITL Patterns with Subgraphs

#### Cross-Subgraph Permission Flow
```python
# Permission requests can span subgraph boundaries
main_graph.add_conditional_edges(
    "research_workflow",
    lambda state: "permission_workflow" if needs_permission(state) else "continue",
    {
        "permission_workflow": "permission_subgraph",
        "continue": "format_response"
    }
)

# Resume after permission granted
main_graph.add_conditional_edges(
    "permission_subgraph",
    lambda state: "research_workflow" if permission_granted(state) else "deny_response",
    {
        "research_workflow": "research_workflow",
        "deny_response": "final_response"
    }
)
```

### Performance Considerations

#### Parallel Subgraph Execution
```python
# Independent subgraphs can run in parallel
async def parallel_subgraph_execution(state):
    research_task = research_subgraph.ainvoke(state)
    analysis_task = analysis_subgraph.ainvoke(state)
    
    # Wait for both to complete
    research_result, analysis_result = await asyncio.gather(
        research_task, analysis_task
    )
    
    # Merge results
    return merge_parallel_results(research_result, analysis_result)
```

#### Subgraph State Optimization
```python
# Minimize state transfer overhead
class OptimizedSubgraphState(TypedDict):
    # Only include essential data for subgraph
    query: str
    context_summary: str  # Instead of full shared_memory
    required_permissions: List[str]  # Instead of full permission state
```

### Error Handling in Subgraphs

#### Contained Error Handling
```python
def subgraph_error_handler(state: SubgraphState) -> SubgraphState:
    try:
        # Subgraph logic here
        result = process_subgraph_logic(state)
        return {"status": "success", "result": result}
    except Exception as e:
        logger.error(f"âŒ SUBGRAPH ERROR: {e}")
        # Return error state, don't propagate
        return {
            "status": "error",
            "error_message": str(e),
            "fallback_result": get_fallback_result(state)
        }
```

### Subgraph Testing Strategies

#### Independent Subgraph Testing
```python
# Test subgraphs in isolation
async def test_research_subgraph():
    test_state = ResearchState(
        query="test query",
        context={"test": "data"},
        permissions={"web_search": True}
    )
    
    result = await research_subgraph.ainvoke(test_state)
    assert result["status"] == "completed"
    assert "findings" in result
```

#### Integration Testing
```python
# Test subgraph integration with main graph
async def test_main_graph_with_subgraphs():
    test_input = {"messages": [HumanMessage(content="research request")]}
    
    # Test full flow including subgraph interactions
    result = await main_graph.ainvoke(test_input)
    assert result["is_complete"] == True
```

## Frontmatter Management

### Always Use Frontmatter Utilities
- **NEVER use regex or string manipulation** to update frontmatter
- **ALWAYS use** [orchestrator/utils/frontmatter_utils.py](mdc:llm-orchestrator/orchestrator/utils/frontmatter_utils.py)
- These utilities preserve all existing fields and handle complex YAML structures

**Required Pattern:**
```python
from orchestrator.utils.frontmatter_utils import add_to_frontmatter_list
from orchestrator.tools.document_tools import get_document_content_tool, update_document_content_tool

# Get content
content = await get_document_content_tool(document_id, user_id)

# Update frontmatter (preserves all existing fields)
updated_content, success = await add_to_frontmatter_list(
    content=content,
    list_key="components",
    new_items=["./new_file.md"]
)

# Save if successful
if success:
    await update_document_content_tool(document_id, updated_content, user_id, append=False)
```

**See [agent-tools-reference.mdc](mdc:.cursor/rules/agent-tools-reference.mdc) for complete frontmatter utility documentation.**

## File Organization

### Agent Files
- Keep agent implementations in [backend/services/langgraph_agents/](mdc:backend/services/langgraph_agents/)
- Use descriptive names: `research_agent.py`, `chat_agent.py`, `coding_agent.py`
- Split large agents (>500 lines) into focused modules

### Subgraph Files
- Create dedicated subgraph modules in [backend/services/langgraph_subgraphs/](mdc:backend/services/langgraph_subgraphs/)
- Use clear naming: `research_subgraph.py`, `permission_subgraph.py`, `analysis_subgraph.py`
- Include state transformation utilities in subgraph modules

### Tool Files  
- Organize tools by domain in [backend/services/langgraph_tools/](mdc:backend/services/langgraph_tools/)
- Provide both class and async function interfaces
- Register tools with descriptive names and clear documentation

### Model Files
- Define all Pydantic models in [backend/models/](mdc:backend/models/)
- Use typed models for agent inputs/outputs
- Include validation and serialization logic
- **Create subgraph-specific state models** in [backend/models/subgraph_models.py](mdc:backend/models/subgraph_models.py)

## Debugging and Monitoring

### Structured Logging
```python
logger.info(f"ðŸ¤– AGENT: {agent_name} processing: {task_summary}")
logger.info(f"ðŸ›‘ HITL: Permission requested for: {operation_type}")  
logger.info(f"âœ… PERMISSION: User approved: {operation_type}")
logger.error(f"âŒ ERROR: {agent_name} failed: {error_details}")
```

### State Inspection
- Log state keys and structure for debugging
- Track agent_results and shared_memory changes
- Monitor conversation flow and routing decisions

## Common Patterns and Utilities

### gRPC Client Management
When agents need backend tools via gRPC:
```python
async def _get_grpc_client(self):
    """Get or create gRPC client for backend tools"""
    if self._grpc_client is None:
        from orchestrator.clients.backend_tool_client import get_backend_tool_client
        self._grpc_client = await get_backend_tool_client()
    return self._grpc_client
```

### Active Editor Access Pattern
```python
async def _prepare_context_node(self, state: YourState) -> Dict[str, Any]:
    metadata = state.get("metadata", {})
    shared_memory = state.get("shared_memory", {})
    active_editor = shared_memory.get("active_editor", {})
    
    if not active_editor:
        return {
            "error": "No active editor found",
            "task_status": "error"
        }
    
    editor_content = active_editor.get("content", "")
    frontmatter = active_editor.get("frontmatter", {})
    # ... use editor data
```

### Reference File Loading Pattern
```python
from orchestrator.tools.reference_file_loader import load_referenced_files

async def _load_references_node(self, state: YourState) -> Dict[str, Any]:
    active_editor = state.get("shared_memory", {}).get("active_editor", {})
    user_id = state.get("user_id")
    
    result = await load_referenced_files(
        active_editor=active_editor,
        user_id=user_id,
        reference_config={"style": True, "characters": True}
    )
    
    return {"referenced_context": result.get("loaded_files", {})}
```

### Structured Response Parsing Pattern
```python
def _parse_json_response(self, content: str) -> Dict[str, Any]:
    """Parse JSON response from LLM, handling markdown code blocks"""
    import json
    import re
    
    # Remove markdown code blocks if present
    json_text = content.strip()
    if '```json' in json_text:
        match = re.search(r'```json\s*\n(.*?)\n```', json_text, re.DOTALL)
        if match:
            json_text = match.group(1).strip()
    elif '```' in json_text:
        match = re.search(r'```\s*\n(.*?)\n```', json_text, re.DOTALL)
        if match:
            json_text = match.group(1).strip()
    
    try:
        return json.loads(json_text)
    except json.JSONDecodeError as e:
        logger.warning(f"Failed to parse JSON response: {e}")
        return {"task_status": "error", "error_message": f"Invalid JSON: {str(e)}"}
```

## Anti-Patterns to Avoid

### âŒ DON'T DO THESE:
- **String matching for agent decisions** - use structured outputs
- **Multiple orchestrators** - use one official orchestrator
- **Custom memory stores** - use LangGraph checkpointing
- **Manual state juggling** - let LangGraph handle flow mechanics
- **Hardcoded routing logic** - use intent classification
- **Monolithic agent files** - split into focused modules
- **Unsafe tool access** - implement permission checks
- **Overuse subgraphs** - don't create subgraphs for simple operations
- **Complex state transformations** - prefer shared state schemas when possible
- **Nested subgraph chains** - avoid subgraphs calling other subgraphs deeply
- **Direct ChatOpenAI creation** - always use `_get_llm()` from BaseAgent
- **chat_service.openai_client usage** - use centralized `_get_llm()` method
- **Missing checkpointer compilation** - always compile with `checkpointer=checkpointer`
- **Skipping message history loading** - always use `_load_and_merge_checkpoint_messages()`
- **Ignoring shared_memory from checkpoints** - always merge checkpointed shared_memory
- **WRONG checkpoint merge order** - letting stale checkpoint overwrite fresh metadata (causes stale content bug!)
- **Nodes that raise exceptions** - nodes should return error states, not raise
- **Missing entry point** - always set entry point with `workflow.set_entry_point()`
- **Missing edges** - all nodes must be connected with edges or conditional edges
- **Dropping state keys in nodes** - nodes must return all critical state keys to preserve them
- **Subgraph nodes dropping metadata** - causes LLM to fall back to default model instead of user's choice!
- **Subgraph nodes missing critical 5 keys** - metadata, user_id, shared_memory, messages, query
- **Not testing with user model preference** - subgraph may silently use wrong model
- **Assuming returns exit graphs** - they don't! Use conditional routing to END
- **Blindly overwriting state with defaults** - preserve existing values first
- **Only preserving critical 5** - preserve ALL workflow-specific state keys
- **Nodes continuing when should stop** - route to END explicitly for early completion

### âœ… DO THESE INSTEAD:
- **Structured Pydantic models** for all agent communication
- **Single source of truth** orchestrator
- **PostgreSQL checkpointing** for persistence
- **LangGraph interrupt_before** for HITL
- **Smart routing** based on query analysis
- **Modular agent architecture** with clear responsibilities
- **Permission-aware tool registry** with access control
- **Strategic subgraph usage** - for complex, reusable workflows only
- **Clear state contracts** - well-defined interfaces between graphs
- **Parallel subgraph execution** - when workflows are independent
- **Centralized LLM access** - always use `_get_llm()` from BaseAgent
- **Proper workflow structure** - nodes, entry point, edges, checkpointer compilation
- **State persistence** - load and merge checkpointed messages and shared_memory
- **Error handling in nodes** - return error states, don't raise exceptions
- **Consistent naming** - follow `_*_node` pattern for all node methods
- **Standard process method** - implement `process()` with checkpoint management
- **CORRECT checkpoint merge order** - always let fresh metadata overwrite stale checkpoint (see section 8a!)
- **Explicit state preservation** - each node returns all critical state keys (active_editor, shared_memory, messages, etc.)
- **ALL subgraph nodes preserve the 5 critical keys** - metadata, user_id, shared_memory, messages, query (in success AND error paths!)
- **Test with user model preference** - verify subgraphs use correct model, not default
- **Subgraph state checklist** - every node, every path, all 5 keys
- **Conditional routing to END** - when work is complete, route directly to END
- **Preserve before defaulting** - check `state.get(key)` before setting defaults
- **Audit ALL workflow state** - identify and preserve every key any node might set
- **Debug with state logging** - track state keys through graph execution

**Remember: A well-organized LangGraph system ensures every agent knows its role and executes it perfectly.**

Follow these practices and your LangGraph agents will operate with maximum efficiency and reliability.